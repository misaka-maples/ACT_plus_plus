
<p>23年已过35 今24年则将36，到40岁之前还有4年半，这4年半我想冲一把<a href="https://edu.csdn.net/cloud/pm_summit?utm_source=blogglc" target="_blank" class="hl hl-1" data-report-click="{&quot;spm&quot;:&quot;1001.2101.3001.7020&quot;,&quot;dest&quot;:&quot;https://so.csdn.net/so/search?q=%E5%A4%A7%E6%A8%A1%E5%9E%8B&amp;spm=1001.2101.3001.7020&quot;,&quot;extra&quot;:&quot;{\&quot;searchword\&quot;:\&quot;大模型\&quot;}&quot;}" data-tit="大模型" data-pretit="大模型">大模型</a><a href="https://so.csdn.net/so/search?q=%E6%9C%BA%E5%99%A8%E4%BA%BA&amp;spm=1001.2101.3001.7020" target="_blank" class="hl hl-1" data-report-click="{&quot;spm&quot;:&quot;1001.2101.3001.7020&quot;,&quot;dest&quot;:&quot;https://so.csdn.net/so/search?q=%E6%9C%BA%E5%99%A8%E4%BA%BA&amp;spm=1001.2101.3001.7020&quot;,&quot;extra&quot;:&quot;{\&quot;searchword\&quot;:\&quot;机器人\&quot;}&quot;}" data-tit="机器人" data-pretit="机器人">机器人</a>(<em>兼具商业价值、社会价值、科技价值&nbsp;</em>)，因为</p> 
<ul><li>通过过去一年的研究探索与应用开发(<em>比如我带队开发完成的<a class="link-info" href="https://aigc.julyedu.com/" rel="nofollow" title="AIGC模特生成">AIGC模特生成</a>、<a class="link-info" href="https://blog.csdn.net/v_JULY_v/article/details/134183799" title="论文审稿GPT">论文审稿GPT</a>、<a class="link-info" href="https://blog.csdn.net/v_JULY_v/article/details/135257848" title="企业知识库问答">企业知识库问答</a>等</em>)，机器人是在可能范围之内我能做的最大的项目(<em>至于更大的如造车 我也干不了</em>)，很难，4年半下来也不一定能达到预期，但全力</li><li>通过Q1之内的技术准备、复现Moblie aloha「<em><span style="color:#7b7f82;">不过，后续4月份改成<strong>先和合作伙伴去复现UMI</strong>和DexCap了，且<a class="link-info" href="https://blog.csdn.net/v_JULY_v/article/details/135515947" title="UMI">UMI</a>和<a class="link-info" href="https://blog.csdn.net/v_JULY_v/article/details/139410045" title="DexCap">DexCap</a>已经复现成功</span></em>」、建机器人开发团队之后，Q2之内将在七月官网上线一系列机器人课程以从课程中筛选、选拔、扩大整个机器人的开发队伍(<em><span style="color:#fe2c24;">比如：<a class="link-info" href="https://www.julyedu.com/course/getDetail/506" rel="nofollow" title="大模型机器人二次开发线下营 [实操20万的实体机器人]">大模型机器人二次开发线下营 [实操20万的实体机器人]</a></span></em>)</li></ul> 
<p>根据<span style="color:#ed7976;">上一篇文章《<a class="link-info" href="https://blog.csdn.net/v_JULY_v/article/details/135429156" title="模仿学习的集中爆发：从Dobb·E、Gello到斯坦福Mobile ALOHA、UMI、DexCap、伯克利FMB">模仿学习的集中爆发：从Dobb·E、Gello到斯坦福Mobile ALOHA、UMI、DexCap、伯克利FMB</a>》的第三部分</span>可知，无论是最新系统Mobile <span class="words-blog hl-git-1" data-tit="ALOHA" data-pretit="aloha">ALOHA</span>还是其前身系统ALOHA，他们背后的关键技术都是动作分块算法ACT，故</p> 
<ul><li>本文侧重阐述ACT的算法原理</li><li>至于ACT的代码剖析、部署实践，请见下一篇文章《<a class="link-info" href="https://blog.csdn.net/v_JULY_v/article/details/135566948" title="逐行解读ACT：斯坦福Mobile Aloha之动作分块算法ACT的代码剖析、训练部署">逐行解读ACT：斯坦福Mobile Aloha之动作分块算法ACT的代码剖析、训练部署</a>》</li></ul> 
<p></p> 
<h2><a name="t1"></a>第一部分 ALOHA + ACT</h2> 
<h3><a name="t2"></a>1.1&nbsp;ALOHA + ACT解决现有机器人昂贵且难以做精确任务的问题</h3> 
<p>斯坦福Mobile ALOHA在被推出之前，其实在23年Q1便已有了ALOHA，所谓ALOHA，即是<strong>A</strong> <strong>L</strong>ow-cost <strong>O</strong>pen-source <strong>Ha</strong>rdware System for Bimanual Teleoperation</p> 
<p>如此文《<a class="link-info" href="https://blog.csdn.net/v_JULY_v/article/details/135429156" title="模仿学习的集中爆发：从Dobb·E、Gello到斯坦福Mobile ALOHA、UMI、DexCap、伯克利FMB">模仿学习的集中爆发：从Dobb·E、Gello到斯坦福Mobile ALOHA、UMI、DexCap、伯克利FMB</a>》的第三部分“3.1.2 Mobile ALOHA的前置工作：ALOHA与ACT”节所述</p> 
<blockquote> 
 <p>Mobile&nbsp;ALOHA其实是在23年ALOHA的工作基础上迭代优化出来的，不是一蹴而就，以下是关于ALOHA的一系列重要信息</p> 
 <ul><li>ALOHA项目地址：<a href="https://tonyzhaozh.github.io/aloha/" rel="nofollow" title="Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware">Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware</a></li><li>论文地址：<a href="https://arxiv.org/abs/2304.13705" rel="nofollow" title="Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware">Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware</a><br> 这是<a href="https://blog.csdn.net/v_JULY_v/article/details/135454242" title="其解读">其解读</a>，论文中首次系统阐述了作为“无论是最新系统Mobile ALOHA还是其前身系统ALOHA中的关键技术”：即动作分块算法ACT<br> 该论文作者包括：Tony Z. Zhao, Vikash Kumar, Sergey Levine, Chelsea Finn</li><li>代码地址：<a href="https://github.com/tonyzhaozh/aloha" title="GitHub - tonyzhaozh/aloha">GitHub - tonyzhaozh/aloha</a><br><em>该代码仓库友情提醒：<br> To build ALOHA, follow the Hardware Assembly Tutorial and the quick start guide below.<br> To train imitation learning algorithms, you would also need to install ACT.</em></li><li>硬件安装指南：<a href="https://docs.google.com/document/d/1sgRZmpS7HMcZTPfGy3kAxDrqFMtNNzmK-yVtX5cKYME/edit" rel="nofollow" title="ALOHA 🏖️ Tutorial">ALOHA 🏖️ Tutorial</a>&nbsp;(文档标题为：<em>ALOHA 🏖️: A Low-cost Open-source Hardware for Bimanual Teleoperation</em>)</li><li>基于动作分块算法ACT的训练代码：<a href="https://github.com/tonyzhaozh/act" title="GitHub - tonyzhaozh/act">GitHub - tonyzhaozh/act</a> <p class="img-center"><img alt="" height="207" src="https://i-blog.csdnimg.cn/blog_migrate/015e7cfb5e205133ec3a791d46c6bba2.png" width="600"></p> </li></ul> 
</blockquote> 
<p>所以，一个低成本的<a href="https://edu.csdn.net/cloud/pm_summit?utm_source=blogglc" target="_blank" class="hl hl-1" data-report-click="{&quot;spm&quot;:&quot;1001.2101.3001.7020&quot;,&quot;dest&quot;:&quot;https://so.csdn.net/so/search?q=%E5%BC%80%E6%BA%90&amp;spm=1001.2101.3001.7020&quot;,&quot;extra&quot;:&quot;{\&quot;searchword\&quot;:\&quot;开源\&quot;}&quot;}" data-tit="开源" data-pretit="开源">开源</a>硬件系统ALOHA，可手动远程操作，是怎么一步步来的呢？</p> 
<p>在前两年，让机器人去完成一些抽象精细的操作任务，比如穿线扎带或开槽电池，是比较困难的</p> 
<ul><li>一方面，因为这些任务需要比较高的精准度、协调性以及闭环视觉反馈。通常情况下，执行这些任务需要使用高端机器人、精确传感器或者仔细校准设备，而且成本昂贵且难以设置<br><br> Stanford University、UC Berkeley、Meta等研究者为了让低成本和不太精确的硬件也能完成这些复杂操作，提出了一个低成本系统，该系统可以通过<strong>定制远程操作接口</strong>收集实际演示，从而进行端到端的模仿学习(<span style="color:#7b7f82;"><em>We present a low-cost system that performs <strong>end-to-end imitation learning</strong> directly from real demonstrations,collected with a <strong>custom teleoperation interface</strong></em></span>) <p class="img-center"><img alt="" height="321" src="https://i-blog.csdnimg.cn/blog_migrate/c7ff76035a30d526a25dab873df4db4b.png" width="1000"></p> </li><li>二方面，在高精度领域中进行模仿学习还存在挑战：策略中的错误可能会随着时间推移而累积，并且人类演示可能是非平稳的<br> 为了解决这些挑战，该团队开发了一种动作分块算法，即Action Chunking with Transformers (ACT)，它基于Transformer在动作序列上生成模型并允许机器人学习现实世界中6项困难任务(<em>例如打开半透明调味品杯和插入电池</em>)，成功率达80-90%(<a class="link-info" href="http://tonyzhaozh.github.io/aloha" rel="nofollow" title="这是其演示地址">这是其演示地址</a>)</li></ul> 
<p>总之，根据该团队发表的这篇论文《Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware》可知</p> 
<ol><li>他们训练了一个端到端的策略，该策略直接将来自商品网络相机的RGB图像映射到动作(<span style="color:#7b7f82;"><em>we therefore train an end-to-end policy that directly maps RGB images from commodity web cameras to the actions.</em></span>)<br> 这种从像素到动作的转换方法特别适用于精细操作，因为精细操纵通常涉及具有复杂物理特质的对象，这样学习操纵策略比建模整个环境要简单得多</li><li>以调味品杯为例：模拟轻推杯子时的接触，以及撬开盖子时的变形，都会引起大量的物理变化。与其耗费大量的研究和具体任务的工程努力去设计一个足够精确的模型进行具体的规划，不如转化为执行上要简单很多的策略制定，即确定轻推和打开杯子的策略或方法，因为策略可以对杯子和盖子的不同位置做出反应，而非提前精确预测它将如何移动<br><span style="color:#7b7f82;"><em>Designing a model accurate enough for planning would require significant research and task specific engineering efforts. In contrast, the policy of nudging and opening the cup is much simpler, since a closed-loop policy can react to different positions of the cup and lid rather than precisely anticipating how it will move in advance</em></span><br><br> 说白了，咱们不是去生成一个轻推和打开杯子的视频，不用去描绘过程，咱们只是要完成一个具体的目标(结果说话)，而完成某个具体的目标有策略、有方法就行</li></ol> 
<h4><a name="t3"></a>1.1.1 <a href="https://so.csdn.net/so/search?q=%E6%A8%A1%E4%BB%BF%E5%AD%A6%E4%B9%A0&amp;spm=1001.2101.3001.7020" target="_blank" class="hl hl-1" data-report-view="{&quot;spm&quot;:&quot;1001.2101.3001.7020&quot;,&quot;dest&quot;:&quot;https://so.csdn.net/so/search?q=%E6%A8%A1%E4%BB%BF%E5%AD%A6%E4%B9%A0&amp;spm=1001.2101.3001.7020&quot;,&quot;extra&quot;:&quot;{\&quot;searchword\&quot;:\&quot;模仿学习\&quot;}&quot;}" data-report-click="{&quot;spm&quot;:&quot;1001.2101.3001.7020&quot;,&quot;dest&quot;:&quot;https://so.csdn.net/so/search?q=%E6%A8%A1%E4%BB%BF%E5%AD%A6%E4%B9%A0&amp;spm=1001.2101.3001.7020&quot;,&quot;extra&quot;:&quot;{\&quot;searchword\&quot;:\&quot;模仿学习\&quot;}&quot;}" data-tit="模仿学习" data-pretit="模仿学习">模仿学习</a>及其挑战：Action Chunking with <span class="words-blog hl-git-1" data-report-view="{&quot;spm&quot;:&quot;1001.2101.3001.10283&quot;,&quot;extra&quot;:&quot;{\&quot;words\&quot;:\&quot;Transformers\&quot;}&quot;}" data-tit="Transformers" data-pretit="transformers">Transformers</span>(ACT)</h4> 
<p id="t-page1-s0-c0-b0-p0">系统有了，数据也好办(<em>高质量的人类演示可以让系统学习人类的灵巧，因此，可让低成本但灵巧的遥操作系统</em>ALOHA做<em>数据收集</em>)，但训练一个端到端的策略(<em><span style="color:#000000;">end-to-end policy</span></em>)可没那么容易，因为即使是高质量的演示，在处理需要精度和视觉反馈的任务时，对模仿学习来说也是一个重大挑战</p> 
<ol><li id="t-page1-s0-c0-b2-p0">预测动作中的小误差会引起状态的大差异，加剧模仿学习的“复合误差”问题。为了解决这个问题，他们从动作分块(action chunking)中获得灵感，这是心理学中的一个概念，描述了如何将一系列动作组合在一起作为一个块，最终作为一个单元执行<br><span style="color:#7b7f82;"><em>Small errors in the predicted action can incur large differences in the state, exacerbating the “compounding error” problem of imitation learning [47, 64, 29]. To tackle this, we take inspiration from action chunking, a concept in psychology that describes how sequences of actions are grouped together as a chunk, and executed as one unit [35].</em></span></li><li>在他们的案例中，策略预测了接下来k个时间步的目标关节位置，而不仅仅是一次一步。这通过k折减少了任务的有效视界，减轻了复合误差<br><span style="color:#7b7f82;"><em>In our case, the policy predicts the target joint positions for the next k timesteps, rather than just one step at a time. This reduces the effective horizon of the task by k-fold, mitigating compounding errors. </em></span><br><br> 总之，预测动作序列也有助于解决时间相关的干扰因素，例如难以用马尔可夫单步策略建模的演示中的停顿<br><span style="color:#7b7f82;"><em>Predicting action sequences also helps tackle temporally correlated confounders [61], such as pauses in demonstrations that are hard to model with Markovian single-step policies.</em></span></li><li>为了进一步提高策略的平滑性，本文提出了时间集成，更频繁地查询策略，并在重叠的动作块上进行平均<br><span style="color:#7b7f82;"><em>To further improve the smoothness of the policy, we propose temporal ensembling, which queries the policy more frequently and averages across the overlapping action chunks.</em></span><br><br> 他们<strong>使用Transformers实现动作分块策略</strong>，并将其训练为<strong>条件VAE</strong> (CVAE)，以捕获人类数据中的可变性。他们将该方法命名为Action Chunking with Transformers(ACT)，并发现它在一系列模拟和现实世界的精细操作任务上显著优于以前的模仿学习算法<br><span style="color:#7b7f82;"><em>We implement <strong>action chunking policy with Transformers</strong> [65], an architecture designed for sequence modeling, and train it as a <strong>conditional VAE</strong> (CVAE) [55, 33] to capture the variability in human data. We name our method Action Chunking with Transformers (ACT), and find that it significantly outperforms previous imitation learning algorithms on a range of simulated and real-world fine manipulation tasks.</em></span></li></ol> 
<h4><a name="t4"></a>1.1.2 行为克隆(Behavioral cloning, BC)中为何要引入ACT</h4> 
<p id="t-page1-s0-c1-b1-p0">模仿学习可以让机器人直接向专家学习，而行为克隆(Behavioral cloning, BC)是最简单的模仿学习算法之一，将模仿作为从观察到行动的监督学习</p> 
<ol><li id="t-page1-s0-c1-b2-p0">BC的一个主要缺点是复合误差，以前时间步长的误差累积并导致机器人偏离其训练分布，导致难以恢复状态[47,64]。这个问题在精细操作设置[29]中尤为突出。减轻复合错误的一种方法是允许额外的政策上的交互和专家修正，如DAgger[47]及其变体[30,40,24]</li><li>然而，对于遥操作界面[29]，专家注释可能是耗时且不自然的。人们也可以在演示采集时注入噪声以获得具有纠正行为[36]的数据集，但对于精细操作而言，这种噪声注入会直接导致任务失败，降低遥操作系统的灵巧度<br><br> 为了规避这些问题，之前的工作以离线的方式生成合成校正数据[16,29,70]。虽然它们仅限于可获得低维状态的设置，或特定类型的任务，如抓取<br> 总之，由于这些限制，需要从不同的角度来解决复合误差问题，好与高维视觉观察兼容</li><li>因此建议通过动作分块(action chunking)来减少任务的有效视界，即预测一个动作序列而不是单个动作，然后跨重叠的动作块进行集成，以产生既准确又平滑的轨迹</li></ol> 
<h3><a name="t5"></a>1.2 硬件套装：ALOHA——低成本的开源硬件系统，用于手动远程操作</h3> 
<p>我们已经知道市面上已有类似达芬奇外科手术机器人或ABB YuMi这样的机器人，但成本一般比较昂贵</p> 
<ul><li>他们团队则转向低成本硬件(<span style="color:#7b7f82;"><em>很类似七月在一系列大模型应用上的探索，侧重小团队 低成本 大效果</em></span>)，例如每个成本约为5k美元的手臂，并寻求使它们能够执行高精度、闭环任务<br> 他们的遥操作设置最类似于Kim等人的[32]，也使用了人类遥控者和跟随者机器人之间的关节空间映射</li><li>与之前的系统不同，他们没有使用特殊的编码器、传感器或加工部件。他们只使用现成的机器人和少量3D打印部件来构建他们的系统，以好让非专业人士在不到2小时内组装好它</li></ul> 
<p>具体而言，其具备以下五个特点</p> 
<ol><li id="t-page2-s1-c0-b3-p0">低成本：整个系统应该在大多数机器人实验室的预算之内，可与单个工业手臂相媲美</li><li id="t-page2-s1-c0-b3-p1">通用性：它可以应用于广泛的与现实物体的精细操作任务</li><li id="t-page2-s1-c0-b3-p2">人性化：系统应该直观、可靠、易于使用</li><li id="t-page2-s1-c0-b3-p3">可修复：当设置不可避免地出现故障时，研究人员可以轻松修复设置</li><li id="t-page2-s1-c0-b3-p4">易于搭建：研究人员可以快速组装，材料来源容易</li></ol> 
<p id="t-page2-s1-c1-b1-p0">所以，在设计一个远程操作系统时，没有将VR控制器或摄像头捕获的手部姿势映射到机器人的末端执行器姿势，即任务空间映射(<span style="color:#7b7f82;"><em>Instead of mapping the hand pose captured by a VR controller or camera to the end-effector pose of the robot, i.e. task-space mapping</em></span>)，而是使用来自同一家公司制造的小型机器人WidowX的直接关节空间映射，成本为3300美元</p> 
<p id="t-page2-s1-c0-b4-p0">如下图所示：</p> 
<p class="img-center"><img alt="" height="330" src="https://i-blog.csdnimg.cn/blog_migrate/115485854b4046f62f250e1f26198e2b.png" width="1200"></p> 
<ul><li>左侧为前、顶部和两个手腕摄像机的视角(<em>这4个相机的视角分别用<span style="color:#4da8ee;">从当前往后的蓝线</span>、<span style="color:#98c091;">从顶向下的绿线</span>、<span style="color:#ed7976;">从左往右的红线</span>、</em><span style="color:#ed7976;"><em>从右往左的红线</em></span><em>表示</em>)，以及ALOHA双手工作空间的示意图<br><br><span style="color:#7b7f82;">具体而言，总计4个Logitech C922x网络摄像头，每个流输出480×640 RGB图像<br><img alt="\rightarrow" class="mathcode" src="https://latex.csdn.net/eq?%5Crightarrow">&nbsp; 其中两个网络摄像头安装在跟随机器人手腕上，以提供夹具的近距离视角(<em>allowing for a close-up view of the grippers</em>)<br><img alt="\rightarrow" class="mathcode" src="https://latex.csdn.net/eq?%5Crightarrow">&nbsp; 剩下的两个相机分别安装在桌面的前方(front camera)和桌子上方的顶部位置(top camera)，遥控操作和数据记录均以50Hz频率进行</span></li><li>中间是“手柄和剪刀”机制和定制夹具的详细视图</li><li>根据上面的原则1、4和5，建立了一个双手平行颚夹持器设置与两个ViperX 6-DoF机器人手臂，上图右侧列出了ViperX 6dof机器人的技术规格<br> 出于价格和维护方面的考虑，不使用灵巧手，使用的ViperX臂具有750克和1.5米跨度的工作有效载荷，精度为5-8毫米</li></ul> 
<p>且该机器人模块化，维修简单：在电机出现故障的情况下，低成本的Dynamixel电机可以轻松更换。这种机器人可以以5600美元左右的价格购买到现货</p> 
<p>然而，OEM的手指不够通用，无法处理精细的操作任务。因此，设计了自己的3D打印“透明”手指，并将其贴合在夹持胶带(<em><span style="color:#000000;">gripping tape</span></em>)上</p> 
<blockquote> 
 <p>再补充说明下</p> 
 <hr> 
 <p>该系统中，用户通过反向驱动较小的WidowX(“领导者”)来远程操作，其关节与较大的ViperX(“追随者”)同步。在开发设置时，注意到使用关节空间映射(<em><span style="color:#000000;">joint-space mapping</span></em>)比使用任务空间有一些好处</p> 
 <ol><li>精细操作通常需要在机器人的<span style="color:#000000;">singularities</span>附近操作，在他们的例子中，机器人有6个自由度，没有冗余。现成的逆运动学(<em><span style="color:#000000;">inverse kinematics，</span>IK</em>)在这种情况下经常失效<br> 另一方面，关节空间映射保证了关节限制内的高带宽控制，同时也需要更少的计算和减少延迟</li><li>leader机器人的重量可以防止用户移动过快，同时也可以抑制小的振动。注意到使用关节空间映射比手持VR控制器在精确任务上表现更好<br> 为了进一步提高遥操作体验，设计了一个3d打印的“手柄和剪刀(<em><span style="color:#000000;">handle and scissor</span></em>)”机构，可以改装到领导者机器人上<br> 它减少了操作人员反向驱动电机所需的力，并允许连续控制夹持器，<span style="color:#000000;">instead of binary opening or closing</span></li><li><span style="color:#000000;">We also design a rubber band load balancing mechanism that partially counteracts the gravity on the leader side</span>。它减少了操作人员所需的努力，并使更长的遥操作时间(如&gt;30分钟)成为可能(<em><a class="link-info" href="https://tonyzhaozh.github.io/aloha/" rel="nofollow" title="项目网站">项目网站</a>中包含了有关设置的更多细节</em>)</li></ol> 
 <p id="t-page2-s1-c1-b2-p0">其余设置包括一个尺寸为20×20mm的铝挤压机器人笼，由交叉钢缆加固(<em>The rest of the setup includes a robot cage with 20×20mm aluminum extrusions, reinforced by crossing steel cables</em>)</p> 
</blockquote> 
<h3><a name="t6"></a></h3> 
<h2><a name="t7"></a>第二部分 动作分块算法ACT的原理解析</h2> 
<h3><a name="t8"></a>2.1 整体概览：从整体流程上整体理解：Action Chunking with Transformers(ACT)</h3> 
<p>为了在新任务上训练ACT，首先使用ALOHA收集人类演示</p> 
<p class="img-center"><img alt="" height="276" src="https://i-blog.csdnimg.cn/blog_migrate/fffa408dde9faa700056ec736aaad3c9.png" width="1000"></p> 
<ul><li>该团队记录了领导机器人的关节位置(<em>record the joint positions ofthe leader robots，即来自人类操作员的输入</em>)，并将其作为动作<br> 之所以使用领导者的关节位置而不是跟随者的关节位置，是因为通过低级PID控制器施加力量时，这些力量是由“它们之间的差异”隐式定义决定的<br><span style="color:#7b7f82;"><em>It is important to use <strong>the leader joint</strong> positions instead of the follower’s, because the amount of force appliedis implicitly defined by the difference between them, through the low-level PID controller</em></span><br><br><span style="color:#ed7976;">观察结果由跟随机器人当前关节位置和4个摄像机图像馈入组成</span>(<span style="color:#7b7f82;"><em>The observations are composed ofthe current joint positions of <strong>follower robots</strong> and the image feedfrom 4 cameras</em></span>)</li><li>接下来，训练ACT根据当前观察结果预测未来动作序列(<span style="color:#7b7f82;"><em>we train ACT to predict the&nbsp;sequence offuture actions&nbsp;given the current observations</em></span>)<br> 这里一个动作对应于下一个时间步中两只手的目标关节位置(<span style="color:#7b7f82;"><em>An action here corresponds to <strong>the target joint positions for both arms</strong> in the next time step</em></span>)<br><br> 直观地说，ACT试图模仿人类操作员在当前观察结果下，在接下来的时间步中会做什么(<span style="color:#7b7f82;"><em>ACT tries to imitate what a human operator would do in the following time steps given current observations</em></span>)</li><li>然后，Dynamixelmotors内部的低级、高频PID控制器跟踪这些目标关节位置。在测试时，加载实现了最低验证损失策略，并在环境中推出。出现的主要挑战是复合错误，其中包括来自之前动作的错误导致了超出训练分布范围的状态<br><span style="color:#7b7f82;"><em>These target joint positions are then tracked bythe low-level, high-frequency PID controller inside Dynamixelmotors.&nbsp;At test time, we load the policy that achieves the lowest validation loss and roll it out in the environment.&nbsp;Themain challenge that arises is compounding errors, where errorsfrom previous actions lead to states that are outside of training distribution.</em></span></li></ul> 
<h4><a name="t9"></a>2.1.1 动作分块：<strong>将同一时间步内的预测动作进行聚合</strong></h4> 
<p>为了以一种与<strong>像素到动作策略兼容</strong>的方式来解决模仿学习中的复合错误，寻求减少高频收集的长轨迹的有效<strong>视域</strong>(<span style="color:#7b7f82;"><em>To combat the compounding errors of imitation learning in away that is compatible with <strong>pixel-to-action policies</strong>,we seek to reduce the effective <strong>horizon</strong> of long trajectories collected at high frequency</em></span>)</p> 
<p>对此，受到了动作分块的启发，这是一个神经科学概念，其中个体动作被分组在一起并作为一个单元执行，从而更有效地存储和执行[<span style="color:#7b7f82;"><em>We are inspired by action chunking,a neuro science concept where individual actions are grouped together and executed as one unit, making them more efficientto store and execute</em></span>]</p> 
<p class="img-center"><img alt="" height="382" src="https://i-blog.csdnimg.cn/blog_migrate/57cec88e6a945900883fdae5d9e5290e.png" width="600"></p> 
<ol><li><u><strong>动作分块</strong></u><br> 直观地说，一组动作可以对应于抓住糖果包装纸角或将电池插入槽中。如上图所示，将块大小固定为k：<span style="color:#ed7976;">每k步agent接收一个观察，并生成下一组k个动作，然后依次执行这些动作</span>。这意味着任务的有效视界减少了k倍<br><span style="color:#7b7f82;"><em>wefix the chunk size to be&nbsp;k: every&nbsp;k&nbsp;steps, the agent receivesan observation, generates the next&nbsp;k&nbsp;actions, and executes theactions in sequence.This implies a&nbsp;k-fold reduction in the effective horizon of the task.</em></span><br><br> 具体来说，该策略模拟<img alt="\pi_{\theta}\left(a_{t: t+k} \mid s_{t}\right)" class="mathcode" src="https://latex.csdn.net/eq?%5Cpi_%7B%5Ctheta%7D%5Cleft%28a_%7Bt%3A%20t&amp;plus;k%7D%20%5Cmid%20s_%7Bt%7D%5Cright%29">而不是<img alt="\pi_{\theta}\left(a_{t} \mid s_{t}\right)" class="mathcode" src="https://latex.csdn.net/eq?%5Cpi_%7B%5Ctheta%7D%5Cleft%28a_%7Bt%7D%20%5Cmid%20s_%7Bt%7D%5Cright%29">。换言之，单步策略将对抗与时间相关的干扰因素，例如人类演示的过程中间出现暂停&nbsp;，因为行为不仅取决于状态还取决于时间步长。通过采用动作分块方法可以缓解这种混淆<br><span style="color:#7b7f82;"><em>Concretely, the policymodels&nbsp;πθ(at:t+k|st)&nbsp;instead of&nbsp;πθ(at|st).&nbsp;Specifically, a single-step policy would struggle with temporally correlated confounders, such as pauses in the middle of ademonstration [61], since the behavior not only depends onthe state, but also the time step.&nbsp;Action chunking can mitigate</em></span><br><br> 这样做可以使不同的动作块相互重叠，<strong>在给定的时间步长上产生多个预测动作</strong>(<span style="color:#7b7f82;"><em>This makes different action chunks overlap with each other,and at a given time step there will be more than one predict edaction</em></span>)，并提出了一个时间集成方法来组合这些预测结果</li><li><strong><u>时间集成</u></strong><br> 此外，其时间集成通过加权平均对这些预测进行处理，采用指数加权方案<img alt="w_{i}=\exp (-m * i)" class="mathcode" src="https://latex.csdn.net/eq?w_%7Bi%7D%3D%5Cexp%20%28-m%20*%20i%29">，其中<br><img alt="\rightarrow" class="mathcode" src="https://latex.csdn.net/eq?%5Crightarrow">&nbsp;&nbsp;<img alt="w_{0}" class="mathcode" src="https://latex.csdn.net/eq?w_%7B0%7D">表示最早动作的权重(<span style="color:#7b7f82;"><em>Our&nbsp;temporal ensemble performs a weighted average over these predictions with anexponential weighting scheme&nbsp;wi&nbsp;= exp(−m&nbsp;∗i), where&nbsp;w0is the weight for the oldest action.&nbsp;</em></span>)<br><br><img alt="\rightarrow" class="mathcode" src="https://latex.csdn.net/eq?%5Crightarrow">&nbsp; 新观察到达时速度由参数<img alt="m" class="mathcode" src="https://latex.csdn.net/eq?m">决定，较小的<img alt="m" class="mathcode" src="https://latex.csdn.net/eq?m">意味着更快地纳入新观察(<span style="color:#7b7f82;"><em>The speed for incorporatingnew observation is governed by&nbsp;m, where a smaller&nbsp;m&nbsp;meansfaster incorporation</em></span>)<br><br> 需要注意的是，与典型平滑方法不同(<span style="color:#7b7f82;"><em>当前动作与相邻时间步中的动作被聚合在一起，即：A B C D 4个动作，那样会引入偏差</em></span>)，<strong>他们只将同一时间步内的预测动作进行聚合</strong>：即<em>D G J M&nbsp;</em>4个动作<br><span style="color:#7b7f82;"><em>We note that unlike typical smoothing,where the current action is aggregated with actions in adjacent time steps, which leads to bias, <strong>we aggregate actions predicted for the&nbsp;same&nbsp;time step</strong>.&nbsp;</em></span><br><br><strong>啥意思呢，如上图所示<br><span style="color:#7b7f82;">在t=0时生成了t=0,1,2,<u>3</u>，四个动作步骤，比如&nbsp; A B C <u>D</u><br> 在t=1时生成了&nbsp; &nbsp;t=1,2,<u>3</u>,4，四个动作步骤，比如&nbsp; &nbsp;E F <u>G</u> H<br> 在t=2时生成了&nbsp; &nbsp; &nbsp; t=2,<u>3</u>,4,5，四个动作步骤，比如&nbsp; &nbsp; I&nbsp; <u>J</u>&nbsp; K L<br> 在t=3时生成了&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;t=<u>3</u>,4,5,6，四个动作步骤，比如&nbsp; &nbsp; <u>M</u> N O P</span><br> 在t=3时最终采用什么动作，由t=0,t=1,t=2,t=3这四段进行指数加权平均，来得到最终的结果</strong><br><br><span style="color:#1a439c;">是不有点像下象棋，每往前走一步，都要推算下未来的4步应该怎么走？为何，因为战局可能瞬息万变，每时每刻都要根据实际情况做出及时调整，而非按照最开始预测的一条道走到黑，^_^</span><br><br> 该过程没有额外训练成本，只需增加推理计算所需时间。实践证明，动作分块和时间集成对ACT模型都至关重要，它们能够产生准确且平滑流畅的运动效果<br><span style="color:#7b7f82;"><em>This procedure also incurs no additional training cost, only extra inference-time computation.&nbsp;In practice,we find both action chunking and temporal ensembling to beimportant for the success of ACT, which produces precise andsmooth motion</em></span></li></ol> 
<h4><a name="t10"></a>2.2.2 Modeling human data与ACT的执行</h4> 
<p>上文先是解决了硬件系统的问题，后又解决了算法优化的问题，接下来，该细化数据的问题了</p> 
<p>实话说，让机器人从嘈杂的人类演示中学习并不容易，面对相同的观察，人类可能会使用不同的轨迹来完成任务。在精度要求较低的区域，人类的行为也会更加随机</p> 
<p>但策略需要关注高精度至关重要的区域，故ALOHA团队不得不通过训练他们的<strong>动作分块策略当做一个生成模型</strong>来解决这个问题(<span style="color:#7b7f82;"><em>We tackle this problem by training our action chunking policy as a generative model</em></span>)</p> 
<ol><li>最终他们将策略训练当做条件变分自编码器的生成问题「<span style="color:#7b7f82;"><em>即we train the policy as a conditional variational autoencoder (CVAE)，而<strong>CVAE用类似BERT的编码器实现</strong></em></span>」，以生成以当前观察为条件的动作序列</li><li>CVAE有两个组件：一个CVAE编码器和一个CVAE解码器，<span style="color:#ad720d;">CVAE编码器只用于训练CVAE解码器(策略)，在测试时被丢弃</span></li></ol> 
<p>具体而言，如下图所示(注意，左侧是CVAE编码器，右侧是CVAE解码器)</p> 
<p class="img-center"><img alt="" height="276" src="https://i-blog.csdnimg.cn/blog_migrate/fffa408dde9faa700056ec736aaad3c9.png" width="1000"></p> 
<ol><li><span style="color:#ad720d;">上图左侧的CVAE 编码器</span>(采用类似BERT的transformer编码器实现)，其预测样式变量 z 的分布的均值和方差，该分布被参数化为对角高斯分布<br> 其<span style="color:#ed7976;">输入</span>是来自演示数据集的<u>当前关节位置</u>，和<u>长度为<img alt="k" class="mathcode" src="https://latex.csdn.net/eq?k">的目标动作序列</u>，前面再加上一个习得的类似于BERT中的<u>“[CLS]”token</u>，从而形成了一个<img alt="k+2" class="mathcode" src="https://latex.csdn.net/eq?k&amp;plus;2">长度的输入<br><span style="color:#7b7f82;"><em>The inputs to the encoder are the current joint positions and the target action sequence of length&nbsp;k&nbsp;from thedemonstration dataset, prepended by a learned “[CLS]” tokensimilar to BERT.&nbsp;</em></span><br><br> 顺带解释一下，为了在实践中加快训练速度，他们省略了图像观测，仅依赖于本体感觉观测和动作序列「<span style="color:#7b7f82;"><em>For faster training in practice, we leave outthe image observations and <strong>only condition on the proprioceptive observation and the action sequence</strong></em></span>」<br><br> 通过<span style="color:#ad720d;">编码器</span>之后，使用“[CLS]”对应的特征用于预测“风格变量”<img alt="z" class="mathcode" src="https://latex.csdn.net/eq?z">的均值和方差，这相当于CVAE 编码器的<span style="color:#1a439c;">输出</span>(<em>当然，其同时也是CVAE解码器的输入之一</em>)</li><li><span style="color:#1c7892;">上图右侧的</span><strong><span style="color:#1c7892;">CVAE解码器</span>(即策略)，通过z和当前观测(图像+机器人关节位置)的条件来预测动作序列</strong>(<em>即接下来的k个动作</em>)<br> 在测试时，将<img alt="z" class="mathcode" src="https://latex.csdn.net/eq?z">设置为先验分布的均值，即0，以确定性的解码「<span style="color:#7b7f82;"><em>啥意思呢，我再解释一下，其实<img alt="z" class="mathcode" src="https://latex.csdn.net/eq?z">代表希望从策略中引发的动作序列的“风格”。可以简单地将z设置为零向量，即训练期间使用的单位高斯先验的均值。因此，给定一个观测，策略的输出始终是确定性的，有利于策略评估，避免语料之外的迷乱行为</em></span>」<br><span style="color:#7b7f82;"><em><strong>The CVAE decoder, i.e.the policy, conditions on both&nbsp;z&nbsp;and the current observations(images + joint positions) to predict the action sequence</strong>.<br> At test time, we set&nbsp;z&nbsp;to be the mean of the prior distribution i.e.zero to deterministically decode.</em></span><br><br> 且他们<u>使用ResNet图像编码器、transformer encoder，和transformer decoder来实现CVAE解码器</u>，直观地说<br><img alt="\rightarrow" class="mathcode" src="https://latex.csdn.net/eq?%5Crightarrow">&nbsp; 其中的transformer编码器综合了来自不同相机视角、关节位置和风格变量<img alt="z" class="mathcode" src="https://latex.csdn.net/eq?z">的信息(<span style="color:#7b7f82;"><em>different camera viewpoints, the joint positions, and the style variable，不知细心的你有没发现，</em></span><span style="color:#1a439c;"><em>这里</em></span><span style="color:#7b7f82;"><em>和</em></span><span style="color:#ad720d;"><em>CVAE编码器中的transformer编码器</em></span><span style="color:#7b7f82;"><em>不同的是，</em></span><span style="color:#1a439c;"><em>前者包含相机视角</em></span><span style="color:#7b7f82;"><em>，</em></span><span style="color:#ad720d;"><em>后者忽略相机视角——即上面说的图像观测</em></span>)<br><img alt="\rightarrow" class="mathcode" src="https://latex.csdn.net/eq?%5Crightarrow">&nbsp; 而其中的transformer解码器生成连贯的动作序列(<span style="color:#7b7f82;"><em>generates a coherent action sequence</em></span>)<br><br> 整个模型被训练成maximize演示动作块的对数似然，即(是一个VAE目标函数)<br><img alt="\min _{\theta}-\sum_{s_{t}, a_{t: t+k} \in D} \log \pi_{\theta}\left(a_{t: t+k} \mid s_{t}\right)" class="mathcode" src="https://latex.csdn.net/eq?%5Cmin%20_%7B%5Ctheta%7D-%5Csum_%7Bs_%7Bt%7D%2C%20a_%7Bt%3A%20t&amp;plus;k%7D%20%5Cin%20D%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D%5Cleft%28a_%7Bt%3A%20t&amp;plus;k%7D%20%5Cmid%20s_%7Bt%7D%5Cright%29"><br><span style="color:#7b7f82;"><em>The whole model is trained to maximize P the log-likelihood of demonstration action chunks, i.e.minθ−st,at:t+k&nbsp;∈D&nbsp;log&nbsp;πθ(at:t+k&nbsp;|st)</em></span><br><br> 该标准的VAE目标函数有两项：重建损失项、和一个将编码器正则化为高斯先验的项。直观地说，较高的β将导致<img alt="z" class="mathcode" src="https://latex.csdn.net/eq?z">中传递的信息减少<br><span style="color:#7b7f82;"><em>with the standard VAEobjective which has two terms: a reconstruction loss and a termthat regularizes the encoder to a Gaussian prior.&nbsp;Following [23],we weight the second term with a hyperparameter&nbsp;β.&nbsp;</em></span></li></ol> 
<h3><a name="t11"></a>2.2 深入细节：从ACT执行的详细架构图与详细步骤进一步理解ACT</h3> 
<p>以下是论文中关于上述训练过程的更多细节</p> 
<p class="img-center"><img alt="" height="276" src="https://i-blog.csdnimg.cn/blog_migrate/fffa408dde9faa700056ec736aaad3c9.png" width="1000"></p> 
<h4><a name="t12"></a><strong>2.2.1 第一步 采样数据：以获得CVAE 编码器的输入中的关节位置和动作序列</strong></h4> 
<p>如下图所示</p> 
<p class="img-center"><img alt="" height="181" src="https://i-blog.csdnimg.cn/blog_migrate/0228296a20f029cc692da6a6357c7d86.png" width="1000"></p> 
<ul><li>输入：包括4张RGB图像，每张图像的分辨率为480&nbsp;×640，以及两个机器人手臂的关节位置(总共7+7=14 DoF)<br><span style="color:#7b7f82;"><em>Intuitively, the transformer encoder synthesizes information from different camera viewpoints, the joint positions, and the style variable, and the transformer decoder generates a coherentaction sequence.&nbsp;The observation includes 4 RGB images, eachat&nbsp;480&nbsp;×640&nbsp;resolution, and joint positions for two robot arms(7+7=14 DoF in total).&nbsp;</em></span></li><li>输出：动作空间是两个机器人的绝对关节位置，一个14维向量<br> 因此，通过动作分块，策略在给定当前观测的情况下输出一个k&nbsp;×14张量(<span style="color:#7b7f82;"><em>每个动作都被定义为一个14维的向量，所以k个动作自然便是一个k&nbsp;×14张量</em></span>)<br><span style="color:#7b7f82;"><em>The action space is the absolute jointpositions for two robots, a 14-dimensional vector.&nbsp;Thus withaction chunking, the policy outputs a&nbsp;k&nbsp;×14&nbsp;tensor giventhe current observation.&nbsp;</em></span></li></ul> 
<h4><a name="t13"></a><strong>2.2.2 第二步 推断z，以获得CVAE解码器输入中的</strong>风格变量z</h4> 
<p>然后，使用下图右侧黄色所示的CVAE编码器推断风格变量z</p> 
<p class="img-center"><img alt="" height="284" src="https://i-blog.csdnimg.cn/blog_migrate/b76f2bb5493f35a82083748caf472b19.png" width="1000"></p> 
<p>通过上一小节，可知<span style="color:#ad720d;">CVAE 编码器</span>的输入目前包括了</p> 
<ol><li>[CLS]token，它由随机初始化的学习权值组成</li><li><strong>嵌入关节位置embedded joints</strong><br> 通过一个线性层linear layer2，把joints投影到嵌入维度的关节位置(<em>14维到512维</em>)，得到：embedded joints</li><li><strong>嵌入动作序列embedded action sequence</strong><br> 通过另一个线性层linear layer1，把k × 14的action sequence投影到嵌入维度的动作序列(<span style="color:#7b7f82;"><em>k × 14维到k ×&nbsp;512维</em></span>)</li></ol> 
<p>以上三个输入最终形成(k + 2)×embedding_dimension的序列，即(k + 2) × 512，<span style="color:#ad720d;">并用CVAE 编码器中的transformer编码器进行处理</span></p> 
<p>最后</p> 
<ol><li>只取第一个输出，它对应于[CLS]标记，并使用另一个线性网络来预测<img alt="z" class="mathcode" src="https://latex.csdn.net/eq?z">分布的均值和方差，将其参数化为对角高斯分布</li><li>且使用重新参数化获得<img alt="z" class="mathcode" src="https://latex.csdn.net/eq?z">的样本，这是一种允许在采样过程中反向传播的标准方法，以便编码器和解码器可以联合优化[33]</li></ol> 
<h4><a name="t14"></a><strong>2.2.3 第三步 </strong>CVAE解码器<strong>预测动作序列</strong></h4> 
<p>接下来，尝试从CVAE解码器中获得预测的动作，即策略(预测动作序列)</p> 
<p class="img-center"><img alt="" height="359" src="https://i-blog.csdnimg.cn/blog_migrate/c05b4ff04cf0b22eb74882581f65790e.png" width="1000"></p> 
<ol><li>首先，对于每一个图像观察，其皆被<strong><em>ResNet18</em></strong>处理以获得一个特征图(<span style="color:#7b7f82;"><em>将480×640×3&nbsp;RGB图像转换为15×20×728的特征图，即which convert 480 × 640 × 3 RGB images into 15 × 20 × 512 feature maps</em></span>)<br> 然后<span style="color:#000000;">flatten</span>化以获得一个特征序列(<em>300×728</em>)<br> 这些特征用线性层linear layer5投影到嵌入维度(<em>300×512</em>)<br> 为了保留空间信息，再添加一个2D正弦位置嵌入(<em>即Sinusoidal PosEmb</em>)，相当于把位置信息添加到特征序列中<br><br> 其次，对所有4张图像重复此操作，得到一个4 × 300 × 512，即1200 ×512维度的特征序列<br><span style="color:#7b7f82;"><em>Repeating this for all 4 images gives a feature sequence of 1200 × 512 in dimension.</em></span><br><br> 接着，将来自每个摄像机的特征序列连接起来，用作<span style="color:#1a439c;">CVAE解码器中transformer encoder</span>的输入之一<br> 对于另外两个输入：当前的关节位置joints和“风格变量”z，它们分别通过线性层linear layer6、linear layer7从各自的原始维度(14、32)都统一投影到512<br><br> 最终，<span style="color:#1a439c;">the input to the transformer encoder</span> is 1202×512(<span style="color:#7b7f82;">相信你很快反应出来了，连接此三：<em>4张图像的特征维度1200 ×512、关节位置joins的特征维度1×512，风格变量z的特征维度1×512</em></span>)</li><li><span style="color:#1a439c;">CVAE解码器中transformer解码器</span>的输入有两个方面<br> 一方面，transformer解码器的“<strong>query</strong>”是第一层固定的正弦位置嵌入，即如上图右下角所示的position embeddings(fixed)，其维度为k ×512<br> 二方面，transformer解码器的交叉注意力(cross-attention)层中的“<strong>keys</strong>”和“<strong>values</strong>”来自上述transformer编码器的输出<br><span style="color:#7b7f82;"><em>即如论文中所述，The transformer decoder conditions on the encoder output through cross-attention, where the input sequence is a fixed position embedding, with dimensions k × 512, and the keys and values are coming from the encoder.</em></span><br><br> 从而，transformer解码器在给定编码器输出的情况下预测动作序列<br> 以下是更多细节<br><span style="color:#7b7f82;"><img alt="\rightarrow" class="mathcode" src="https://latex.csdn.net/eq?%5Crightarrow">&nbsp; transformer解码器的输出维度是k ×512，然后用MLP向下投影到k ×14，对应于接下来k个步骤的预测目标关节位置<br><em>This gives the transformer decoder an output dimension of k × 512, which is then down-projected with an MLP into k × 14, corresponding to the predicted target joint positions for the next k steps. </em><br><br><img alt="\rightarrow" class="mathcode" src="https://latex.csdn.net/eq?%5Crightarrow">&nbsp; 最终，使用L1损失进行重建，而不是更常见的L2损失：且注意到，L1损失导致对动作序列进行更精确的建模(<em>We use L1 loss for reconstruction instead of the more common L2 loss: we noted that L1 loss leads to more precise modeling of the action sequence. </em>)<br><br><img alt="\rightarrow" class="mathcode" src="https://latex.csdn.net/eq?%5Crightarrow">&nbsp;&nbsp;且还注意到，当使用delta关节位置作为动作而不是目标关节位置时，性能会下降(<em>We also noted degraded performance when using delta joint positions as actions instead of target joint positions)</em></span></li></ol> 
<blockquote> 
 <p>以下是算法1和算法2中ACT的训练和推理。该模型有大约80M个参数，为每个任务从头开始训练。训练在单个11G RTX 2080 Ti GPU上需要大约5个小时，在同一台机器上的推理时间约为0.01秒</p> 
 <p>&nbsp;<img alt="" height="200" src="https://i-blog.csdnimg.cn/blog_migrate/d8478b3ba9d57240b40795c819a66a56.png" width="363"><img alt="" height="200" src="https://i-blog.csdnimg.cn/blog_migrate/590d41f653fd12a87a8984f92b04e279.png" width="494"></p> 
</blockquote> 
<h3><a name="t15"></a>2.3 优势特征：ACT与其他模仿学习方法的比较</h3> 
<p>将ACT与之前的4种模仿学习方法进行比较</p> 
<ol><li>BC-ConvMLP是最简单但最广泛使用的基线[69, 26]，它采用卷积网络处理当前图像观测，并将其输出特征与关节位置连接，以预测动作<br><span style="color:#7b7f82;"><em>BC-ConvMLP is the simplest yet most widely used baseline [69, 26], which processes the current image observations with a convolutional network, whose output features are concatenated with the joint positions to predict the action.</em></span></li><li>BeT [49]也利用Transformer作为架构，但有两个关键区别：<br><img alt="\rightarrow" class="mathcode" src="https://latex.csdn.net/eq?%5Crightarrow">&nbsp; (1)没有对动作进行分块处理，该模型在给定观测历史的情况下预测一个动作<br><span style="color:#7b7f82;"><em>no action chunking: the model predicts one action given the history of observations</em></span><br><img alt="\rightarrow" class="mathcode" src="https://latex.csdn.net/eq?%5Crightarrow">&nbsp; (2)图像观测由单独训练的<strong>冻结视觉编码器</strong>预处理，即感知和控制网络没有联合优化<br><span style="color:#7b7f82;"><em>the image observations are pre-processed by a separately trained <strong>frozen visual encoder</strong>. That is, the perception and control networks are not jointly optimized.</em></span></li><li>RT-1 [7]是另一种基于Transformer的架构，从“固定长度的<strong>过去观测历史中</strong>”预测一个动作<br><span style="color:#7b7f82;"><em>RT-1 [7] is another Transformerbased architecture that predicts one action from a <strong>fixed-length history of past observations</strong>. </em></span><br><br> BeT和RT-1都离散化了动作空间：输出为离散箱分类分布，在BeT情况下还添加了连续偏移量。而<strong>ACT直接预测连续动作</strong>，这是出于精确操作所需精度驱使<br><span style="color:#7b7f82;"><em>Both BeT and RT-1 discretize the action space: the output is a categorical distribution over discrete bins, but with an added continuous offset from the bincenter in the case of BeT. Our method, ACT, instead <strong>directly predicts continuous actions</strong>, motivated by the precision required in fine manipulation.</em></span></li><li>最后，VINN [42]是一种非参数方法，在测试时假设可以访问演示数据。给定新的观察数据时，它检索具有最相似视觉特征的k个样本，并返回一个加权操作<br><span style="color:#7b7f82;"><em>Lastly, VINN [42] is a non-parametric method that assumes access to the demonstrations at test time. Given a new observation, it retrieves the k observations with<br> the most similar visual features, and returns an action using&nbsp;weighted k-nearest-neighbors.</em></span><br><br> 且其采用了一个经过预训练的ResNet作为视觉特征提取器，并在无监督学习的演示数据上进行微调<br><span style="color:#7b7f82;"><em>The visual feature extractor is a pretrained ResNet finetuned on demonstration data with unsupervised learning.</em></span></li></ol> 
<p>如本文开头所说的，&nbsp;至于ACT的代码剖析、部署实践，请见下一篇文章《<a class="link-info" href="https://blog.csdn.net/v_JULY_v/article/details/135566948" title="逐行解读ACT：机器人模仿学习之动作分块算法ACT的代码剖析">逐行解读ACT：机器人模仿学习之动作分块算法ACT的代码剖析</a>》</p> 
<p></p> 
<h2><a name="t16"></a><strong>&nbsp;参考文献与推荐阅读</strong></h2> 
<ol><li><a href="https://tonyzhaozh.github.io/aloha/" rel="nofollow" title="https://tonyzhaozh.github.io/aloha/">https://tonyzhaozh.github.io/aloha/</a></li><li><a class="link-info" href="https://zhuanlan.zhihu.com/p/677090509" rel="nofollow" title="Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware(阅读笔记)">Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware(阅读笔记)</a></li><li>..</li></ol>
                </div><div data-report-view="{&quot;mod&quot;:&quot;1585297308_001&quot;,&quot;spm&quot;:&quot;1001.2101.3001.6548&quot;,&quot;dest&quot;:&quot;https://blog.csdn.net/v_JULY_v/article/details/135454242&quot;,&quot;extend1&quot;:&quot;pc&quot;,&quot;ab&quot;:&quot;new&quot;}"><div></div></div>
        </div>
![img_1.png](img_1.png)